{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac385a-4503-4990-929b-c82b222115de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f924e12-252d-4453-a92f-fef758a855ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/Tou7and/slp-nutshell/main/text_classification/data/facebook_tsai/positive.txt\n",
    "!wget https://raw.githubusercontent.com/Tou7and/slp-nutshell/main/text_classification/data/facebook_tsai/negative.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388794d5-57c3-4b85-8d1f-d4137cab5261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_unigram(text):\n",
    "    words = list(text)\n",
    "    return words\n",
    "\n",
    "def keep_mandarin(sent):\n",
    "    pattern_zh = re.compile(u'[⺀-⺙⺛-⻳⼀-⿕々〇〡-〩〸-〺〻㐀-䶵一-鿃豈-鶴侮-頻並-龎]')\n",
    "    results = pattern_zh.finditer(sent)\n",
    "\n",
    "    zh_chars = []\n",
    "    for result in results:\n",
    "        # print(result.group(), result.span())\n",
    "        zh_chars.append(result.group())\n",
    "    sent_new = \"\".join(zh_chars)\n",
    "    return sent_new\n",
    "\n",
    "\n",
    "def load_data_from_file(text_file):\n",
    "    with open(text_file, 'r') as reader:\n",
    "        lines = reader.readlines()\n",
    "    \n",
    "    corpus = []\n",
    "    for line in lines:\n",
    "        corpus.append(keep_mandarin(line))\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def load_sentiment_data_from_file(pos_file, neg_file):\n",
    "    \"\"\" Return corpus and corresponding labels \"\"\"\n",
    "    pos_data = load_data_from_file(pos_file)\n",
    "    neg_data = load_data_from_file(neg_file)\n",
    "\n",
    "    # pos_train = pos_data[:len(pos_data)-100]\n",
    "    pos_train = pos_data[:150]\n",
    "    pos_test = pos_data[-100:]\n",
    "    # neg_train = neg_data[:len(neg_data)-100]\n",
    "    neg_train = neg_data[:150]\n",
    "    neg_test = neg_data[-100:]\n",
    "\n",
    "    corpus_train = pos_train + neg_train\n",
    "    labels_train  = [\"pos\"]*len(pos_train) + [\"neg\"]*len(neg_train)\n",
    "    corpus_test = pos_test + neg_test\n",
    "    labels_test  = [\"pos\"]*len(pos_test) + [\"neg\"]*len(neg_test)\n",
    "\n",
    "    dataset = {\n",
    "        \"train\": (corpus_train, labels_train),\n",
    "        \"test\": (corpus_test, labels_test),\n",
    "    }\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dce0a9-5a3b-4556-9cb4-9db52b3034c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Data!\n",
    "\n",
    "dataset = load_sentiment_data_from_file(\n",
    "    \"./positive.txt\",\n",
    "    \"./negative.txt\"\n",
    ")\n",
    "\n",
    "train_corpus, train_labels = dataset[\"train\"]\n",
    "test_corpus, test_labels = dataset[\"test\"]\n",
    "\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_unigram, stop_words=[\"，\",\"。\", \"\\n\", \" \"], min_df=min_df)\n",
    "\n",
    "counts = vectorizer.fit_transform(train_corpus).toarray()\n",
    "\n",
    "# print(vectorizer.get_feature_names_out())\n",
    "# print(counts)\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "clf.fit(counts, train_labels)\n",
    "\n",
    "test_counts = vectorizer.transform(test_corpus).toarray()\n",
    "y_pred = []\n",
    "for kk in test_corpus:\n",
    "    kk_counts = vectorizer.transform([kk]).toarray()\n",
    "    y_pred.append(clf.predict(kk_counts)[0])\n",
    "\n",
    "print(accuracy_score(y_pred, test_labels))\n",
    "if verbose > 0:\n",
    "    print(test_labels)\n",
    "    print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0596b5-58ff-4cfb-a11f-cb86f8a1b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    \"政府實在過於無能\",\n",
    "    \"政府很有效率\",\n",
    "    \"阿不就好棒棒\",\n",
    "    \"索尼罪大惡極 百姓怨聲載道\",\n",
    "]\n",
    "\n",
    "# train_tfidf_gnb()\n",
    "vectorizer, clf = train_bow(feat=\"unigram\", cla=\"dctree\")\n",
    "for sample in samples:\n",
    "    print(sample, clf.predict(vectorizer.transform([sample]).toarray()))\n",
    "\n",
    "vectorizer, clf = train_bow(feat=\"bigram\", cla=\"dctree\")\n",
    "for sample in samples:\n",
    "    print(sample, clf.predict(vectorizer.transform([sample]).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de760f-d206-43bc-9dfb-025c908e10ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
