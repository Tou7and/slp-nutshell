{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eac385a-4503-4990-929b-c82b222115de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f924e12-252d-4453-a92f-fef758a855ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/Tou7and/slp-nutshell/main/text_classification/data/facebook_tsai/positive.txt\n",
    "!wget https://raw.githubusercontent.com/Tou7and/slp-nutshell/main/text_classification/data/facebook_tsai/negative.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388794d5-57c3-4b85-8d1f-d4137cab5261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_unigram(text):\n",
    "    words = list(text)\n",
    "    return words\n",
    "\n",
    "def keep_mandarin(sent):\n",
    "    pattern_zh = re.compile(u'[⺀-⺙⺛-⻳⼀-⿕々〇〡-〩〸-〺〻㐀-䶵一-鿃豈-鶴侮-頻並-龎]')\n",
    "    results = pattern_zh.finditer(sent)\n",
    "\n",
    "    zh_chars = []\n",
    "    for result in results:\n",
    "        # print(result.group(), result.span())\n",
    "        zh_chars.append(result.group())\n",
    "    sent_new = \"\".join(zh_chars)\n",
    "    return sent_new\n",
    "\n",
    "\n",
    "def load_data_from_file(text_file):\n",
    "    with open(text_file, 'r') as reader:\n",
    "        lines = reader.readlines()\n",
    "    \n",
    "    corpus = []\n",
    "    for line in lines:\n",
    "        corpus.append(keep_mandarin(line))\n",
    "    return corpus\n",
    "\n",
    "def load_sentiment_data_from_file(pos_file, neg_file):\n",
    "    \"\"\" Return corpus and corresponding labels \"\"\"\n",
    "    pos_data = load_data_from_file(pos_file)\n",
    "    neg_data = load_data_from_file(neg_file)\n",
    "\n",
    "    # pos_train = pos_data[:len(pos_data)-100]\n",
    "    pos_train = pos_data[:150]\n",
    "    pos_test = pos_data[-100:]\n",
    "    # neg_train = neg_data[:len(neg_data)-100]\n",
    "    neg_train = neg_data[:150]\n",
    "    neg_test = neg_data[-100:]\n",
    "\n",
    "    corpus_train = pos_train + neg_train\n",
    "    labels_train  = [\"pos\"]*len(pos_train) + [\"neg\"]*len(neg_train)\n",
    "    corpus_test = pos_test + neg_test\n",
    "    labels_test  = [\"pos\"]*len(pos_test) + [\"neg\"]*len(neg_test)\n",
    "\n",
    "    dataset = {\n",
    "        \"train\": (corpus_train, labels_train),\n",
    "        \"test\": (corpus_test, labels_test),\n",
    "    }\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dce0a9-5a3b-4556-9cb4-9db52b3034c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "dataset = load_sentiment_data_from_file(\n",
    "    \"./positive.txt\",\n",
    "    \"./negative.txt\"\n",
    ")\n",
    "\n",
    "# Split training and testing set\n",
    "train_corpus, train_labels = dataset[\"train\"]\n",
    "test_corpus, test_labels = dataset[\"test\"]\n",
    "\n",
    "# Check training data\n",
    "print(train_corpus[:5], \"\\n\", train_corpus[-5:])\n",
    "print(train_labels[:5], \"\\n\", train_labels[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac50f0d-e22a-44df-8e0f-c757b58b7067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徵: 使用 scikit learn 的 CountVectorizer (Bag-of-words)\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "\n",
    "# 初始化\n",
    "vectorizer = CountVectorizer(tokenizer=tokenize_unigram, min_df=2)\n",
    "\n",
    "# just fit it\n",
    "vectorizer.fit(train_corpus)\n",
    "\n",
    "# 觀察學到的 feature\n",
    "print(vectorizer.get_feature_names_out().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c59918-e738-4618-a51e-479250b179d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a DecisionTree\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "# 用剛剛得到的 vectorizer 萃取特徵\n",
    "training_feats = vectorizer.transform(train_corpus)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "clf.fit(training_feats, train_labels)\n",
    "\n",
    "test_counts = vectorizer.transform(test_corpus).toarray()\n",
    "y_pred = []\n",
    "for kk in test_corpus:\n",
    "    kk_counts = vectorizer.transform([kk]).toarray()\n",
    "    y_pred.append(clf.predict(kk_counts)[0])\n",
    "\n",
    "# 看看 預測結果 跟 標準答案 的差距\n",
    "print(\"Accuracy on testing set: \", accuracy_score(y_pred, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0596b5-58ff-4cfb-a11f-cb86f8a1b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference some real samples\n",
    "samples = [\n",
    "    \"政府實在過於無能\",\n",
    "    \"政府很有效率\",\n",
    "    \"阿不就好棒棒\",\n",
    "    \"索尼罪大惡極 百姓怨聲載道\",\n",
    "]\n",
    "\n",
    "for sample in samples:\n",
    "    print(sample, clf.predict(vectorizer.transform([sample]).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de760f-d206-43bc-9dfb-025c908e10ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice: Train a better classifier\n",
    "\n",
    "# Hint 1: try new features, ex. bag-of-bigram.\n",
    "# or try any other features you think of or find.\n",
    "def tokenize_bigram(text):\n",
    "    # Try to implement bigram\n",
    "    # Hints: loop through the text\n",
    "    bigrams = []\n",
    "    return bigrams\n",
    "\n",
    "vectorizer2 = CountVectorizer(tokenizer=tokenize_bigram, min_df=2)\n",
    "\n",
    "vectorizer2.fit(train_corpus)\n",
    "\n",
    "training_feats = vectorizer2.transform(train_corpus)\n",
    "# print(vectorizer2.get_feature_names_out().tolist())\n",
    "\n",
    "# Hint 2: try new classifiers\n",
    "# You can replace the Decision Tree with any other classifiers on scikit learn\n",
    "# https://scikit-learn.org/stable/auto_examples/calibration/plot_compare_calibration.html\n",
    "clf2 = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "clf2.fit(training_feats, train_labels)\n",
    "\n",
    "test_counts = vectorizer2.transform(test_corpus).toarray()\n",
    "y_pred = []\n",
    "for kk in test_corpus:\n",
    "    kk_counts = vectorizer2.transform([kk]).toarray()\n",
    "    y_pred.append(clf2.predict(kk_counts)[0])\n",
    "\n",
    "print(\"Accuracy on testing set: \", accuracy_score(y_pred, test_labels))\n",
    "\n",
    "samples = [\n",
    "    \"政府實在過於無能\",\n",
    "    \"政府很有效率\",\n",
    "    \"阿不就好棒棒\",\n",
    "    \"索尼罪大惡極 百姓怨聲載道\",\n",
    "]\n",
    "\n",
    "print()\n",
    "for sample in samples:\n",
    "    print(sample, clf2.predict(vectorizer2.transform([sample]).toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffbecc1-272d-4f79-80a8-898563ddd66d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
